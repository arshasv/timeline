{
  "title": {
    "media": {
      "url": "/images/LLM.jpeg",
      "credit": "Generated image from stable diffusion"
    },
    "text": {
      "headline": "LLM Architecture",
      "text": "<p>Discover the forefront of AI innovation with advanced LLM architecture, designed to drive the future of intelligent systems. Experience the power and precision that sets the standard for tomorrow's technology.</p>"
    }
  },
    "events": [
      {
        "media": {
          "url": "/images/tocken.png",
          "credit": "image from google"
        },
        "start_date": {
          "year": "2024",
          "month": "02",
          "day": "26"
        },
        "text": {
          "headline": "Tocken Generation",
          "text": "<p>Token generation in LLMs powers the foundation of AI language understanding. By breaking down text into manageable units, it enables precise, context-aware processing, ensuring your applications are both intelligent and responsive.</p>"
        }
      },
      {
        "media": {
          "url": "/images/att.jpeg",
          "credit": "image from ideogram"
        },
        "start_date": {
          "year": "2024",
          "month": "02",
          "day": "30"
        },
        "text": {
          "headline": "Attention mechanism",
          "text": "<p>An attention mechanism is a technique used in machine learning and artificial intelligence to improve the performance of models by focusing on relevant information. It allows models to selectively attend to different parts of the input data, assigning varying degrees of importance or weight to different elements.</p>"
        }
      },
      {
        "media": {
          "url": "/images/en.jpeg",
          "credit": "image from ideogram"
        },
        "start_date": {
          "year": "2024",
          "month": "03",
          "day": "03"
        },
        "text": {
          "headline": "Encoder only Architecture",
          "text": "<p> Input is text and output is sequence of embeddings. Use cases are sequence classification (class token), token classification. It uses bidirectional attention, so the model can see forwards and backwards.</p>"
        }
      },
      {
        "media": {
          "url": "/images/de.jpeg",
          "credit": "image from ideogram"
        },
        "start_date": {
          "year": "2024",
          "month": "03",
          "day": "10"
        },
        "text": {
          "headline": "Decoder only Architecture",
          "text": "<p>Input is text and output is the next word (token), which is then appended to the input. Use cases are mostly text generation (autoregressive), but with prompting we can do many things including sequence classification. The attention is almost always causal (unidirectional), so the model can see only previous tokens (prefix).</p>"
        }
      },
      {
        "media": {
          "url": "/images/ende.jpeg",
          "credit": "image from ideogram"
        },
        "start_date": {
          "year": "2024",
          "month": "03",
          "day": "19"
        },
        "text": {
          "headline": "Encoder-Decoder Architecture",
          "text": "<p>The EncoderDecoderModel can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the encoder and any pretrained autoregressive model as the decoder.</p>"
        }
      }
    ]
}
